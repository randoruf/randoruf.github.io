---
layout: post
title: "线性代数的本质"
date: 2021-05-03
tags: [linear_algebra]
---

* TOC
{:toc}

---

要学会一门学科，首先你要掌握**核心思想**。

比如要学会编程， 就要学会图灵机、计算机内存指令等等。

核心思想通常只占20%， 往往其思想很简单。

不要故意通过困难的方法去学习一门课， 要优先思考**核心思想**，再去逐步拓展和延申。 



## 线性代数 Crash Course - 3Blue1Brown 的课程

- Building Blocks 
  - **基**、**标量**
  - **加法**、**乘法**
  - **向量**: 基通过标量的防缩 的 加法组合 $x = a \hat{i} + b\hat{j}$
  - **矩阵**: 多个基 线性变换后 的坐标的集合。
    - 比如 `[a, b; c, d]` 中的 `(a, c)` 是 **第一组基** 在线性变换后的坐标。看 03 - 矩阵和线性变换
    - 关于乘法的理解： 
      - 一个标量和向量相乘
      - 一个矩阵和向量相乘
    - 关于加法的理解：
      - 多个向量的组合
  - 我们明白 **矩阵是线性变换** (移动空间中的向量集合)
    - 准确来说是**对 基底 的线性变换**。
      - (因为任何变量都可以由基底描述，所以只需要知道变换后的基底即可)。 
- 向量的起点 
- 向量空间 (张成的空间)

- 基向量 (如何通过两种线性操作获得整个向量空间)
- **张成** = 通过线性操作组合 (乘法 、加法) **扩张成多个向量的集合**。 
  - 如果 S 中只有零向量，那么其张成的空间也只有零向量。
  - 如果一个向量集包含两个不平行的非零向量，那么其可以张成整个二维平面。
  - 如果一个向量集包括三个线性不相关的非零向量， 那么其可以张成整个三维空间。
  - 具体可以看视频里的 P2
- 基向量的线性相关和线性不相关 (是否可以由里面其他向量**通过线性组合获得**)
  - P2 结尾有两种线性相关的定义。但视频里用第二种。
  - 比如二维空间里，两条平行线，是线性相关。因为可以通过缩放(scaling) 获得。
- "**向量空间**的**一组基**是**张成**该空间的一个**线性无关**的**向量集**"

- ***线性变换***
  - 变换 = 函数。给定**向量输入**，得到**向量输出**。
  - 03 线性变换 01:34  - (还有各种非线性变换)
    - **空间中的直线**依旧是直线(不仅仅是 grid line，比如对角线)
      - 记忆技巧： 空间中的 grid line 是 parallel (平行) 和 evenly spaced (等距分布)
    - **原点**是固定的 (移动了原点叫仿射变换)
  - 线性变换 与 **基**
  - 为什么**线性变换**需要**矩阵** $A$ 和 **输入向量** $x$ （03 - **05:15**） 
    - **二维的线性变换**完全由**四个参数**的矩阵 $A$ 决定 (column 分别代表每个**基向量** 变换后的位置)
    - <img src="/shared/imgs/image-20210720125037481.png" alt="image-20210720125037481" style="zoom:33%;" />
    - $x$ 对应 **第一个基向量 (第一个 column)** ，这就是**向量的乘法**
    - 把**变换矩阵 $A$ 的 columns 看作变换后的基底** (用最少的参数去描述一个线性变换)
    - ***线性变换的本质*** ： 移动空间中的所有向量 (**旋转**、**平移**)。 
      - Linear transformations are methods to **move around space**  such that the grid line remain ***parallel and evenly spaced*** and such that ***origin remain fixed***. 
      - 线性变换矩阵 $A$ 的参数是**变换后基底的坐标**。 
      - 每当你见到**矩阵**时，都可以想象为对**空间(向量集合)** 的**一种特定变换** 。
        - （反复背诵，以后机器人学非常有用）
  - 线性变换例子
    - 旋转变换 03 - **08:18** 
    - Shear 剪切变换 03 - 08: 36 
- ***线性变换的复合***
  - 我们明白 **矩阵是线性变换** (移动空间中的向量集合)
    - 准确来说是**对 基底 的线性变换**。
      - (因为任何变量都可以由基底描述，所以只需要知道变换后的基底即可)。 
  - 但如果两个矩阵相乘呢？这就是线性变换的复合
  - ![image-20210720133500798](/shared/imgs/image-20210720133500798.png)
  - 注意到**矩阵的复合是 “从右到左”** (图中是 先旋转 后剪切)
    - 这是由于要和表示式保持一致， 如 $f(g(x))$ 。见 0**4 - 04: 21** 
  - **矩阵乘法**的新理解 **04 - 05:29** 
    - 关键是思考 $i, j$ 变换后的**坐标**
    - 建议看视频，这里截图占太多空间。 
    -  **不要死记硬背矩阵乘法**
  - 这就很容易解释**矩阵乘法**
    - **不满足交换律** 
      - (可以动手试一下不同顺序的变换，这个 Robotics 也很重要， **到底是先旋转还是先平移**。
      - **一定要先旋转**，否则就不是对着原点旋转.... )
      - 显然 $f(g(x)) \neq g(f(x))$
    - **满足结合率**
      - 用几何可以轻松证明。因为**绝对顺序**没有改变。而且结合也就是另一个矩阵而已。
  - 旋转矩阵(魔方) P6 - 附录 1 - 三维空间中的线性变换
- ***行列式*** 
  - 线性变换的本质是对空间进行**形变** (**拉伸**或**挤压**)
  - 如何测量**形变量**？ - 05 - 行列式 **01:07** 
    - 这里是 **线性形变量**。 
      - 可以理解为在***二维空间里，将空间内的单位正方形的面积缩放一个常数***。 即**缩放后的面积**，看视频 **05 - 08:37** 关于 Determinant 在二维空间是平行四边形的面积。  
      - ***三维空间***就是***单位正方体***的体积。如果拉伸可以变成斜不拉几的的东西（平行六面体）。
    - 注意 **grid cell** 的形变可以用来描述任何图形 (建议看视频 05 )
    - 这个**矩阵的形变量**叫 **Determinant**  (建议看视频 05 )
    - 注意当**空间定向(orientation)** 发生变化时， determinant 为负数，但绝对值依旧表示形变量
    - 当 determinant = 0 时， 被压缩到一个更低维的空间 (例如平面变直线)。
    - （强烈建议看视频 **05 - 05:02** ，非常精彩 。理解**什么是 orientation 发生变化**）
  - ***在三维空间里， Determinant 是对 “单位正方体” 形变后 成为 “平行六面体的体积“。***
    - （强烈建议看视频 **05 - 06:21** ，非常精彩 ）
    - 当 Determinant = 0， 显然是一个平面、一条直线、一个点 (这些东西都没有体积)
    - 而且 Determinant = 0 代表 ***矩阵的 Column 是 线性相关*** (参考第二章， 第三个向量被包含在其他两个向量张成的空间中)。
  - 那么**在三维空间下**，**负的 Determinant** 代表什么。
    - 显然跟 Orientation 有关 (坐标轴的方向)， 用**右手定则**
  - 在三维空间， 平行六面体的体积计算 可以由简单的正方体、三角形组合而成
    - 详情看 视频 **05 - 08:53** 如何通过小学的**补齐**思想来计算面积。体积的道理一样。
  - 课后练习： 如果 Determinant 就是**缩放量**
    - 尝试证明 $det(M_1M_2) = det(M_1) det(M_2)$ 。比如放大了 2 倍，又缩小 0.8 倍，难道不是一样的吗？
- 逆矩阵 (Inverse Matrix)、列矩阵(Column Space)、秩(Rank)、零空间(Null Space)
  - **线性方程组** (不允许出现*非线性函数*) - **06 - 02:16**  。即 $Ax = v$
  - 到这里你明白 $A$ 是线性变化， 而 $v$ 是常数向量
  - 已知**线性变换后的向量**$v$ 和 **描述线性变换的矩阵** $A$， 
    - 我们想知道到底什么向量 $x$ 可以有 $f(x) = v$ 。
    - 其中 $x$ 为**未知量** 
  - 找到**倒带操作** $A^{-1}$ 
  - 如果 Determinant = 0 代表整个空间**被压缩到更低维的空间上**
    - **降维后无法升维** (比如 音乐文件有损压缩太严重 (变成 8 bits)， 就无法恢复(只能通过猜恢复，但猜的话有无数种可能) 到原始格式了)
    - 这就是 **一对多** (并不符合**函数的映射定义**) 
  - 这就是为什么 Determinant = 0 ，**无唯一的逆矩阵** $A^{-1}$
  - 接下来强烈建议看视频 06 。不做笔记了。
  - **Rank** 代表什么？
    - 看视频 **06 - 07:45** 
  - **Column Space** 和 **Rank** 的关系 
    - 看视频 **06 - 08:42** 
    - Rank 是 Column Space 的**维数**
    - 而 Column Space 是**线性变换后** ， 所有 Columns 能够到达的空间
      -  (**每个 Column 代表 基向量变换后的*坐标***)
      - 注意到 **零向量一定在 Column Space** 
      - (变换后的所有**基的原点不动**，可以看上面的 “**平行+均距分布**”和"**原点不动**")
    - **Full Rank** 代表 **Rank 和 列数相等 **(即 空间没有被压缩，Column Space 的维数没有减少)
    - **变换后过原点代表什么**？ 看 **06 - 09:33** 
      - **零空间** ：变换后，**一些向量落在零向量上**，零空间正是由这些**原空间中的向量构成**的空间。 
      - 显然, **Full Rank** 只有 **零向量** 落在 **零空间**
      - 从代数的角度， 如果 $Ax = v$ 的 $v$ 恰好为零向量，那么 $x$ 就是零空间 (原来的)
- 非方矩阵



## 线性代数课程

- [干货 - 万字长文带你复习线性代数！ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/52013163)
- [机器学习理论篇1：机器学习的数学基础 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/25197792)
- [新MIT 线性代数 - 机器学习（中英机翻字幕）18.065 by Gilbert Strang_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1a7411M7wH/)

- [【MIT】线性代数（声道修复）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1bb411H7JN/)
- [【官方双语/合集】线性代数的本质 - 系列合集_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili](https://www.bilibili.com/video/BV1ys411472E)
- [Linear Algebra Done Right - Sheldon Axler - YouTube](https://www.youtube.com/channel/UCtHp0WNe3OaSXAr1C_Oi0AQ/videos)



## 线性代数 - 累积速度为常量

本质实际上是 **批量计算多因素对多因素的累积**。 这就是初中数学
$$
y = Ax + b
$$

- 你可以把矩阵拆解成向量，那就明白批量的意思了
- 如果单独一个观察事实(比如 $$x = [x_1; x_2]$$， 有两个变量代表一个观察事实)， 对应一个多因素的累积（也就是向量对矩阵）。
- 如果继续拆解，就有了向量对向量
- 最终会成为我们理解的一维乘法 。
	- 从**两点连成的线(普通一维乘法)** 拓展到 **多点形成的网络(矩阵和矩阵的乘法)**。
	- **累积速度**是$$A$$, **偏移**是$$b$$ 。  
- 所以线性代数实际上和微积分很类似（**通过乘法快速计算累积量**） 
- 为什么标量的英文叫 Scalar ?
	- 因为在线性代数中，单一数字的通常作用是对向量进行线性变化(比如拉伸、缩短、反方向)， 这种行为叫 Scaling ， 所以单一数字叫Scalar 。

<iframe src="//player.bilibili.com/player.html?aid=32026984&bvid=BV1yW41167ax&cid=62643367&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

当然， 线性代数也有**除法**，**可以用来求累积速度**(或者观察事实)， 本质竟然跟初中代数一模一样。

还记得**逆矩阵**解 $$x$$ 吗？





## 微积分 - 累积速度为变量

如果**累积速度**，即 $$y = Ax +b $$ 中的 $$A$$ 不是常量呢？

我们就无法用线性代数了， 这就是微积分。实际上

- 积分 - 乘法(累积和)
- 微分 - 除法(累积速度)

在线性代数的矩阵分解中，你最终发现， **变量和累积速度的相乘是独立的**(一个变量只会跟其对应的累积速度相乘，不会被别的变量或者别的累积速度干扰)。

在微积分中， 也有类似的概念， **如果累积速度只跟某个变量相关**，那就是**偏导**。

